# SLURM Multi-Server ML Training Demo

A simple demonstration of distributed machine learning training using SLURM orchestration across 4 Docker containers, with all configuration loaded from `config.json`.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Server 1      â”‚    â”‚   Server 2      â”‚    â”‚   Server 3      â”‚    â”‚   Server 4      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ GPU 0     â”‚  â”‚    â”‚   â”‚ GPU 0     â”‚  â”‚    â”‚   â”‚ GPU 0     â”‚  â”‚    â”‚   â”‚ GPU 0     â”‚  â”‚
â”‚   â”‚ ResNet50  â”‚  â”‚    â”‚   â”‚ ResNet50  â”‚  â”‚    â”‚   â”‚ ResNet50  â”‚  â”‚    â”‚   â”‚ ResNet50  â”‚  â”‚
â”‚   â”‚ + DDP     â”‚  â”‚    â”‚   â”‚ + DDP     â”‚  â”‚    â”‚   â”‚ + DDP     â”‚  â”‚    â”‚   â”‚ + DDP     â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚   Docker         â”‚    â”‚   Docker         â”‚    â”‚   Docker         â”‚    â”‚   Docker         â”‚
â”‚   Container      â”‚    â”‚   Container      â”‚    â”‚   Container      â”‚    â”‚   Container      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚                       â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
                    â”‚      SLURM Scheduler      â”‚       â”‚
                    â”‚    (Job Orchestration)    â”‚       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
                                                         â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                    PyTorch DDP Distributed Training                        â”‚
                    â”‚  - 4 GPUs total (1 per server)                                         â”‚
                    â”‚  - Gradient synchronization across all workers                           â”‚
                    â”‚  - Distributed data sampling                                            â”‚
                    â”‚  - Checkpoint management (rank 0 only)                                  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ File Structure

```
orchestration/
â”œâ”€â”€ slurm_training_demo.py      # Main training script
â”œâ”€â”€ Dockerfile                  # Docker image definition
â”œâ”€â”€ submit_slurm_job.sh         # SLURM job submission
â”œâ”€â”€ run_training.sh             # Simple training runner
â”œâ”€â”€ config.json                 # All configuration parameters
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ checkpoints/                # Model checkpoints
â”œâ”€â”€ data/                       # Dataset storage
â””â”€â”€ logs/                       # Training logs
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
cd /workspace/Bara/Notebooks/orchestration
pip install -r requirements.txt
```

### 2. Run Training (Choose one method)

#### Option A: Docker Compose (Local 4 Workers)

**Requirements**: 
- Docker daemon running
- NVIDIA Docker runtime (`nvidia-docker2` or `nvidia-container-toolkit`)
- At least 4 GPUs available

**Note**: If Docker is not available, use Option C.

```bash
# Simple one-command start (recommended)
./run_training_docker.sh

# Or use the management script
./docker_compose_manager.sh start
```

#### Option B: SLURM (Cluster)

```bash
# Submit SLURM job
sbatch submit_slurm_job.sh
```

#### Option C: Local Testing (Single Worker)

```bash
# Test on local machine without Docker
./test_single.sh
```

### 3. Monitor Training

```bash
# Check job status
squeue -u $USER

# View logs
tail -f logs/slurm_<JOB_ID>.out
```

## ğŸ”§ Configuration

All parameters are loaded from `config.json`:

### Model & Training
```json
{
    "training": {
        "model": {
            "name": "resnet50",
            "num_classes": 10,
            "pretrained": false
        },
        "data": {
            "dataset": "cifar10",
            "batch_size": 32,
            "num_workers": 4
        },
        "optimizer": {
            "type": "sgd",
            "learning_rate": 0.1,
            "momentum": 0.9,
            "weight_decay": 1e-4
        },
        "training": {
            "num_epochs": 100,
            "save_frequency": 10
        }
    }
}
```

### SLURM Resources
```json
{
    "slurm": {
        "job_name": "ml_training_demo",
        "partition": "gpu",
        "nodes": 4,
        "ntasks_per_node": 1,
        "gres": "gpu:1",
        "cpus_per_task": 8,
        "mem": "64G",
        "time": "02:00:00"
    }
}
```

### Distributed Training
```json
{
    "distributed": {
        "backend": "nccl",
        "world_size": 4
    }
}
```

**Note**: The training script automatically uses the `gloo` backend if CUDA is not available (e.g., CPU-only environments). On systems with GPUs, it uses `nccl` for faster GPU communication.

## ğŸ“Š Training Process

1. **SLURM Allocation**: 4 nodes Ã— 1 GPU = 4 total GPUs
2. **Docker Startup**: Each node runs Docker container
3. **DDP Initialization**: PyTorch DDP across all containers
4. **Data Distribution**: CIFAR-10 split across workers
5. **Model Training**: ResNet50 with gradient sync
6. **Checkpointing**: Model saved by rank 0

## ğŸ“ˆ Example Output

```
============================================================
SLURM Multi-Server ML Training Demo
============================================================
Job ID: 12345
Job Name: ml_training_demo
Nodes: ml-server-[1-4]
Tasks: 16
Master: ml-server-1:12355
Rank: 0, Local Rank: 0
============================================================

ğŸš€ Starting training...
âœ“ Loaded configuration from config.json
âœ“ Distributed training initialized on rank 0 using nccl
âœ“ Model setup complete on rank 0 - resnet50

Epoch: 0, Batch: 0, Loss: 2.3012, Acc: 10.94%
Epoch: 0, Batch: 100, Loss: 1.8765, Acc: 29.94%
ğŸ“Š Epoch 0 - Time: 47.23s - Train Acc: 29.94%, Val Acc: 43.40%
âœ“ Checkpoint saved: checkpoints/checkpoint_epoch_0.pth

...

âœ… Training completed!
Check checkpoints/ and logs/ directories for results
============================================================
```

## ğŸ§ª Local Testing

For environments without SLURM, use the local testing script:

```bash
# Run single worker test (no distributed training)
./test_single.sh
```

This script:
- Runs training without distributed setup
- Uses single worker mode
- Perfect for testing the training code
- No network or multi-process dependencies

**Note**: For true distributed training across 4 workers, use SLURM cluster with `sbatch submit_slurm_job.sh`.

## ğŸ› ï¸ Management

### Docker Compose Commands

```bash
# Start 4 Docker containers
./docker_compose_manager.sh start

# Stop containers
./docker_compose_manager.sh stop

# View logs
./docker_compose_manager.sh logs

# Check status
./docker_compose_manager.sh status

# Restart containers
./docker_compose_manager.sh restart

# Clean up
./docker_compose_manager.sh clean
```

### SLURM Commands

```bash
# Submit job
sbatch submit_slurm_job.sh

# Check status
squeue -u $USER

# Cancel job
scancel <JOB_ID>

# View details
scontrol show job <JOB_ID>
```

### Docker Commands

```bash
# Build image
docker build -t ml-training .

# Run container (testing)
docker run --gpus all -it ml-training
```

## ğŸ” Troubleshooting

### Common Issues

```bash
# Check SLURM status
sinfo -p gpu

# Check job details
scontrol show job <JOB_ID>

# Check GPU allocation
nvidia-smi

# Enable debug logging
export NCCL_DEBUG=INFO
```

## ğŸ”„ Customization

Edit `config.json` to change:

- **Model**: `"name": "resnet50"` or `"simplecnn"`
- **Dataset**: `"dataset": "cifar10"` or `"imagenet"`
- **Resources**: `"nodes": 4`, `"gres": "gpu:1"`
- **Training**: `"num_epochs": 100`, `"batch_size": 32`

## ğŸ“š Key Features

- âœ… **Simple Setup**: One script to run everything
- âœ… **SLURM Integration**: Professional job scheduling
- âœ… **Docker Containers**: Isolated environments
- âœ… **Config-Driven**: All parameters from config.json
- âœ… **PyTorch DDP**: Distributed training across 4 GPUs
- âœ… **ResNet50**: Production-ready model
- âœ… **Automatic Checkpointing**: Model saving

---

**Usage**: `sbatch submit_slurm_job.sh` - That's it!